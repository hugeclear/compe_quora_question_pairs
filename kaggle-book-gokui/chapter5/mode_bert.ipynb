{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from common import *\n",
    "from experiments import *\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "@contextmanager\n",
    "def simple_timer(message, logger=None):\n",
    "    start_time = time.time()\n",
    "    yield\n",
    "    timer_message = f'{message}: {time.time() - start_time:.3f}  [s]'\n",
    "    if logger is None:\n",
    "        print(timer_message)\n",
    "    else:\n",
    "        logger.info(timer_message)\n",
    "        \n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "def compute_weights(y, target_positive_ratio):\n",
    "    positive_ratio = y.mean()\n",
    "    positive_weight = target_positive_ratio / positive_ratio\n",
    "    negative_weight = (1 - target_positive_ratio) / (1 - positive_ratio)\n",
    "    weights = np.full(len(y), negative_weight)\n",
    "    weights[y == 1] = positive_weight\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('~/src/kaggle/train.csv',na_filter=False)\n",
    "df['weight'] = compute_weights(df.is_duplicate, 0.174)\n",
    "raw_dataset = datasets.Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id', 'qid1', 'qid2', 'question1', 'question2', 'is_duplicate', 'weight']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dataset.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404290, 7)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.arrow_dataset.Dataset"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(raw_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 0,\n",
       " 'qid1': 1,\n",
       " 'qid2': 2,\n",
       " 'question1': 'What is the step by step guide to invest in share market in india?',\n",
       " 'question2': 'What is the step by step guide to invest in share market?',\n",
       " 'is_duplicate': 0,\n",
       " 'weight': 1.3094438628066831}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(examples):\n",
    "    return tokenizer(\n",
    "        examples['question1'],\n",
    "        examples['question2'],\n",
    "        truncation='longest_first',\n",
    "        max_length=max_length,\n",
    "    )\n",
    "max_length=83\n",
    "remove_columns = ['id','qid1','qid2','question1','question2']    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c2644205ac344f7a8569b11081da584",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/404290 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = raw_dataset.map(\n",
    "    preprocess, batched=True, remove_columns=remove_columns\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'is_duplicate': 0,\n",
       " 'weight': 1.3094438628066831,\n",
       " 'input_ids': [101,\n",
       "  2054,\n",
       "  2003,\n",
       "  1996,\n",
       "  3357,\n",
       "  2011,\n",
       "  3357,\n",
       "  5009,\n",
       "  2000,\n",
       "  15697,\n",
       "  1999,\n",
       "  3745,\n",
       "  3006,\n",
       "  1999,\n",
       "  2634,\n",
       "  1029,\n",
       "  102,\n",
       "  2054,\n",
       "  2003,\n",
       "  1996,\n",
       "  3357,\n",
       "  2011,\n",
       "  3357,\n",
       "  5009,\n",
       "  2000,\n",
       "  15697,\n",
       "  1999,\n",
       "  3745,\n",
       "  3006,\n",
       "  1029,\n",
       "  102],\n",
       " 'token_type_ids': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "トークナイザは、通常、特別なトークン（BERTベースのモデルにおける[SEP]のような）でそれらを分離しながら、これら2つのテキストを内部的に連結します。この分離トークンは、1つの質問がどこで終わり、もう1つの質問がどこで始まるかをモデルが理解するのに役立つため、非常に重要です。\n",
    "トークン化：連結された文字列は、トークンのシーケンスに変換されます。トークン化は、トークナイザーの設定に応じて、テキストを単語、サブワード、または文字に分割します。\n",
    "トークンIDに変換します：各トークンは、トークナイザーの語彙から一意の整数IDにマッピングされます。これが input_id と呼ばれるものです。\n",
    "切り捨て：トークンの総数がmax_lengthを超えた場合、指定された戦略（この場合はlongest_first）に基づいてシーケンスが切り捨てられます。\n",
    "パディング：シーケンスがmax_lengthより短い場合、指定された最大長に達するまで特別なパディングトークン（[PAD]など）でパディングされます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-01 02:45:11.314402: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-01 02:45:11.314476: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-01 02:45:11.315773: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-05-01 02:45:12.689882: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "collater_fn = DataCollatorWithPadding(tokenizer=tokenizer,padding='longest')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "data_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=2,\n",
    "    collate_fn = collater_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModelV1(nn.Module):\n",
    "    def __init__(self, model_name_or_path, dropout=0.1):\n",
    "        super(TransformerModelV1, self).__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(mdoel_name_or_path)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear = nn.Linear(self.encoder.config.hidden_size, 1)\n",
    "        \n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None):\n",
    "        outputs = self.encoder(\n",
    "            input_ids=input_ids,\n",
    "            token_type_ids=token_type_ids,\n",
    "            attention_mask=attention_mask,\n",
    "        )\n",
    "        return self.linear(self.dropout(outputs[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_adamw_optimizer(model, learning_rate, weight_decay):\n",
    "    no_decay = ['bias',\"LayNorm.weight\"]\n",
    "    optimizer_parameters = [\n",
    "        {\n",
    "            \"params\":[\n",
    "                p\n",
    "                for n, p in model.named_parameters()\n",
    "                if not any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            'weight_decay':weight_decay,\n",
    "        },\n",
    "        {\n",
    "            \"params\":[\n",
    "                p\n",
    "                for n, p in model.named_parameters()\n",
    "                if any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "    return AdamW(optimizer_parameters, lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trn_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m get_linear_schedule_with_warmup\n\u001b[1;32m      2\u001b[0m \u001b[39m#訓練セット（trn_dataset), エポック数(num_epochs), バッチサイズ（batch_size)から、\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m#学習全体のステップ数を求める\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m num_training_steps \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(trn_dataset) \u001b[39m*\u001b[39m num_epochs\u001b[39m/\u001b[39m\u001b[39m/\u001b[39mbatch_size\n\u001b[1;32m      7\u001b[0m \u001b[39m#学習全体のステップ数とウォームアップあっぷ期間の全学習機関に対する割合（warmup_step_ratio)から、\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[39m# ウォームアップ期間の学習ステップ数を求める\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[39m# warmup_step_ratioは0以上0.2以下の値が指定されることが多い\u001b[39;00m\n\u001b[1;32m     10\u001b[0m num_warmup_steps \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(num_training_steps \u001b[39m*\u001b[39m warmup_step_ratio)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trn_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "#訓練セット（trn_dataset), エポック数(num_epochs), バッチサイズ（batch_size)から、\n",
    "#学習全体のステップ数を求める\n",
    "num_training_steps = len(trn_dataset) * num_epochs//batch_size\n",
    "\n",
    "\n",
    "#学習全体のステップ数とウォームアップあっぷ期間の全学習機関に対する割合（warmup_step_ratio)から、\n",
    "# ウォームアップ期間の学習ステップ数を求める\n",
    "# warmup_step_ratioは0以上0.2以下の値が指定されることが多い\n",
    "num_warmup_steps = int(num_training_steps * warmup_step_ratio)\n",
    "\n",
    "#最適化アルゴリズムと学習ステップ数に関する情報を元に学習率スケジュールを作成する\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, num_training_steps, num_warmup_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
