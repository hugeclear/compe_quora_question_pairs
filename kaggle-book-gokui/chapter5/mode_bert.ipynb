{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "@contextmanager\n",
    "def simple_timer(message, logger=None):\n",
    "    start_time = time.time()\n",
    "    yield\n",
    "    timer_message = f'{message}: {time.time() - start_time:.3f}  [s]'\n",
    "    if logger is None:\n",
    "        print(timer_message)\n",
    "    else:\n",
    "        logger.info(timer_message)\n",
    "        \n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "def compute_weights(y, target_positive_ratio):\n",
    "    positive_ratio = y.mean()\n",
    "    positive_weight = target_positive_ratio / positive_ratio\n",
    "    negative_weight = (1 - target_positive_ratio) / (1 - positive_ratio)\n",
    "    weights = np.full(len(y), negative_weight)\n",
    "    weights[y == 1] = positive_weight\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('~/src/kaggle/train.csv',na_filter=False)\n",
    "df['weight'] = compute_weights(df.is_duplicate, 0.174)\n",
    "raw_dataset = datasets.Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id', 'qid1', 'qid2', 'question1', 'question2', 'is_duplicate', 'weight']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dataset.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404290, 7)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.arrow_dataset.Dataset"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(raw_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 0,\n",
       " 'qid1': 1,\n",
       " 'qid2': 2,\n",
       " 'question1': 'What is the step by step guide to invest in share market in india?',\n",
       " 'question2': 'What is the step by step guide to invest in share market?',\n",
       " 'is_duplicate': 0,\n",
       " 'weight': 1.3094438628066831}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(examples):\n",
    "    return tokenizer(\n",
    "        examples['question1'],\n",
    "        examples['question2'],\n",
    "        truncation='longest_first',\n",
    "        max_length=max_length,\n",
    "    )\n",
    "max_length=83\n",
    "remove_columns = ['id','qid1','qid2','question1','question2']    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd121d271f8c40c69a165fb4f8426f80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/404290 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = raw_dataset.map(\n",
    "    preprocess, batched=True, remove_columns=remove_columns\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'is_duplicate': 0,\n",
       " 'weight': 1.3094438628066831,\n",
       " 'input_ids': [101,\n",
       "  2054,\n",
       "  2003,\n",
       "  1996,\n",
       "  3357,\n",
       "  2011,\n",
       "  3357,\n",
       "  5009,\n",
       "  2000,\n",
       "  15697,\n",
       "  1999,\n",
       "  3745,\n",
       "  3006,\n",
       "  1999,\n",
       "  2634,\n",
       "  1029,\n",
       "  102,\n",
       "  2054,\n",
       "  2003,\n",
       "  1996,\n",
       "  3357,\n",
       "  2011,\n",
       "  3357,\n",
       "  5009,\n",
       "  2000,\n",
       "  15697,\n",
       "  1999,\n",
       "  3745,\n",
       "  3006,\n",
       "  1029,\n",
       "  102],\n",
       " 'token_type_ids': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1]}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "トークナイザは、通常、特別なトークン（BERTベースのモデルにおける[SEP]のような）でそれらを分離しながら、これら2つのテキストを内部的に連結します。この分離トークンは、1つの質問がどこで終わり、もう1つの質問がどこで始まるかをモデルが理解するのに役立つため、非常に重要です。\n",
    "トークン化：連結された文字列は、トークンのシーケンスに変換されます。トークン化は、トークナイザーの設定に応じて、テキストを単語、サブワード、または文字に分割します。\n",
    "トークンIDに変換します：各トークンは、トークナイザーの語彙から一意の整数IDにマッピングされます。これが input_id と呼ばれるものです。\n",
    "切り捨て：トークンの総数がmax_lengthを超えた場合、指定された戦略（この場合はlongest_first）に基づいてシーケンスが切り捨てられます。\n",
    "パディング：シーケンスがmax_lengthより短い場合、指定された最大長に達するまで特別なパディングトークン（[PAD]など）でパディングされます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-30 18:58:28.001086: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-30 18:58:28.001162: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-30 18:58:28.002429: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-30 18:58:28.995600: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "collater_fn = DataCollatorWithPadding(tokenizer=tokenizer,padding='longest')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trn_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m DataLoader\n\u001b[1;32m      3\u001b[0m data_loader \u001b[39m=\u001b[39m DataLoader(\n\u001b[0;32m----> 4\u001b[0m     trn_dataset,\n\u001b[1;32m      5\u001b[0m     batch_size,\n\u001b[1;32m      6\u001b[0m     shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m      7\u001b[0m     drop_last\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m      8\u001b[0m     num_workers\u001b[39m=\u001b[39mnum_workers,\n\u001b[1;32m      9\u001b[0m     collate_fn \u001b[39m=\u001b[39m collater_fn\n\u001b[1;32m     10\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trn_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "data_loader = DataLoader(\n",
    "    trn_dataset,\n",
    "    batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=num_workers,\n",
    "    collate_fn = collater_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
